<h1 align="center">
<img src="./logo.png" width="100" alt="ToRA" />
<br>
Symbol-LLM: Towards Foundational Symbol-centric Interface for Large Language Models
</h1>

<div align="center">

![](https://img.shields.io/badge/Task-Mathematical%20Reasoning-orange)
![](https://img.shields.io/badge/Model-Released-blue)
![](https://img.shields.io/badge/Code%20License-MIT-green)
<br>
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/tora-a-tool-integrated-reasoning-agent-for/math-word-problem-solving-on-math)](https://paperswithcode.com/sota/math-word-problem-solving-on-math?p=tora-a-tool-integrated-reasoning-agent-for)

</div>



<p align="center">
  <a href="https://xufangzhi.github.io/symbol-llm-page/"><b>[ğŸŒ Website]</b></a> â€¢
  <a href="https://arxiv.org/abs/2311.09278"><b>[ğŸ“œ Paper]</b></a> â€¢
  <a href="https://huggingface.co/Symbol-LLM/Symbol-LLM-7B-Instruct"><b>[ğŸ¤— HF Models]</b></a> â€¢
  <a href="https://github.com/xufangzhi/Symbol-LLM"><b>[ğŸ± GitHub]</b></a>
  <br>
  <a href="https://twitter.com/zhs05232838/status/1708860992631763092"><b>[ğŸ¦ Twitter]</b></a> â€¢
  <a href="https://www.reddit.com/r/LocalLLaMA/comments/1703k6d/tora_a_toolintegrated_reasoning_agent_for/"><b>[ğŸ’¬ Reddit]</b></a> â€¢
  <a href="https://notes.aimodels.fyi/researchers-announce-tora-training-language-models-to-better-understand-math-using-external-tools/">[ğŸ€ Unofficial Blog]</a>
  <!-- <a href="#-quick-start">Quick Start</a> â€¢ -->
  <!-- <a href="#%EF%B8%8F-citation">Citation</a> -->
</p>

<!-- <p align="center">
<img src="./logo.png" width="200" height="200" alt="Symbol-LLM">
</p> -->

It is the project page for the paper: ``Symbol-LLM: Towards Foundational Symbol-centric Interface for Large Language Models``

Paper Link: [Symbol-LLM](https://arxiv.org/pdf/2311.09278v1.pdf)

Project Page: [Symbol-LLM-Page](https://xufangzhi.github.io/symbol-llm-page/)


## Note
The work is still in progress.

We will release Symbol-LLM series models (7B / 13B) and the text-to-symbol task collections.


## Citation
If you find it helpful, please kindly cite the paper.
```
@article{xu2023symbol,
  title={Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models},
  author={Xu, Fangzhi and Wu, Zhiyong and Sun, Qiushi and Ren, Siyu and Yuan, Fei and Yuan, Shuai and Lin, Qika and Qiao, Yu and Liu, Jun},
  journal={arXiv preprint arXiv:2311.09278},
  year={2023}
}
```
